{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alfiya\\Desktop\\SLAB-main\\Code\n"
     ]
    }
   ],
   "source": [
    "cd C:\\Users\\Alfiya\\Desktop\\SLAB-main\\Code"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T08:55:14.711399Z",
     "start_time": "2024-03-23T08:55:14.708141300Z"
    }
   },
   "id": "95311996e749595f",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Alfiya\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#Necessary packages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "\n",
    "from supervised_models import logit, xgb_model, mlp\n",
    "from utils import perf_metric\n",
    "from VIME.vime_self import vime_self\n",
    "from hexr_self import hexr_self"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T08:55:17.878141500Z",
     "start_time": "2024-03-23T08:55:14.712416900Z"
    }
   },
   "id": "848243406e568057",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def one_hot_encode(df, class_label):\n",
    "    #one-hot encode class column\n",
    "    ohe = OneHotEncoder()\n",
    "    #Choose either class1 or class2 to select either valence or arousal\n",
    "    df_ohe = pd.DataFrame(ohe.fit_transform(df[[class_label]]).toarray())\n",
    "    df = df.join(df_ohe)\n",
    "    \n",
    "    df.drop('class', axis=1, inplace=True)\n",
    "    \n",
    "    X = df.loc[:,:'gyr_maxabssum']\n",
    "    y = df.iloc[:,22:]\n",
    "    return X, y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T08:55:17.878141500Z",
     "start_time": "2024-03-23T08:55:17.870715Z"
    }
   },
   "id": "d0e12fb4afbb84aa",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def split_data_with_preprocessing(X, y, label_data_rate, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Train-test split and train split into unlablled and labelled data with standardization and normalization\n",
    "    if Test is 15% of the data\n",
    "    10% of 85% = 8.5% :- Labelled dataset\n",
    "    90% of 85% = 76.5% :- Unlabelled dataset\n",
    "    \n",
    "    if Test is 20% of the data \n",
    "    10% of 80% = 8% :- Labelled dataset\n",
    "    90% of 80% = 72% :- Unlabelled dataset\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "    norm_scaler = MinMaxScaler()\n",
    "\n",
    "    X[['acc_x_mean', 'acc_x_var', 'acc_y_mean', 'acc_y_var', 'acc_z_mean', 'acc_z_var', 'acc_sum_mean', 'acc_abssum_mean', 'acc_sum_var', 'acc_abssum_var', 'acc_maxabssum','gyr_x_mean', 'gyr_x_var', 'gyr_y_mean', 'gyr_y_var', 'gyr_z_mean', 'gyr_z_var', 'gyr_sum_mean', 'gyr_abssum_mean', 'gyr_sum_var', 'gyr_abssum_var', 'gyr_maxabssum']] = norm_scaler.fit_transform(X[['acc_x_mean', 'acc_x_var', 'acc_y_mean', 'acc_y_var', 'acc_z_mean', 'acc_z_var', 'acc_sum_mean', 'acc_abssum_mean', 'acc_sum_var', 'acc_abssum_var', 'acc_maxabssum','gyr_x_mean', 'gyr_x_var', 'gyr_y_mean', 'gyr_y_var', 'gyr_z_mean', 'gyr_z_var', 'gyr_sum_mean', 'gyr_abssum_mean', 'gyr_sum_var', 'gyr_abssum_var', 'gyr_maxabssum']])\n",
    "    \n",
    "    std_scaler = StandardScaler()\n",
    "    \n",
    "    X[['acc_x_mean', 'acc_x_var', 'acc_y_mean', 'acc_y_var', 'acc_z_mean', 'acc_z_var', 'acc_sum_mean', 'acc_abssum_mean', 'acc_sum_var', 'acc_abssum_var', 'acc_maxabssum','gyr_x_mean', 'gyr_x_var', 'gyr_y_mean', 'gyr_y_var', 'gyr_z_mean', 'gyr_z_var', 'gyr_sum_mean', 'gyr_abssum_mean', 'gyr_sum_var', 'gyr_abssum_var', 'gyr_maxabssum']] = std_scaler.fit_transform(X[['acc_x_mean', 'acc_x_var', 'acc_y_mean', 'acc_y_var', 'acc_z_mean', 'acc_z_var', 'acc_sum_mean', 'acc_abssum_mean', 'acc_sum_var', 'acc_abssum_var', 'acc_maxabssum','gyr_x_mean', 'gyr_x_var', 'gyr_y_mean', 'gyr_y_var', 'gyr_z_mean', 'gyr_z_var', 'gyr_sum_mean', 'gyr_abssum_mean', 'gyr_sum_var', 'gyr_abssum_var', 'gyr_maxabssum']])\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=33) \n",
    "    \n",
    "    #converting to numpy arrays\n",
    "    x_train = x_train.iloc[:, :].values\n",
    "    y_train = y_train.iloc[:, :].values\n",
    "    x_test = x_test.iloc[:,:].values\n",
    "    y_test = y_test.iloc[:,:].values\n",
    "    x_train.shape, x_test.shape, y_train.shape, y_test.shape\n",
    "    \n",
    "    # Divide labeled and unlabeled data\n",
    "    idx = np.random.permutation(len(y_train))\n",
    "    \n",
    "    # Label data : Unlabeled data = label_data_rate:(1-label_data_rate)\n",
    "    label_idx = idx[:int(len(idx)*label_data_rate)]\n",
    "    unlab_idx = idx[int(len(idx)*label_data_rate):]\n",
    "    \n",
    "    # Unlabeled data\n",
    "    x_unlab = x_train[unlab_idx, :]\n",
    "    \n",
    "    # Labeled data\n",
    "    x_train = x_train[label_idx, :] \n",
    "    y_train = y_train[label_idx, :]\n",
    "    return x_unlab, x_train, x_test, y_train, y_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T08:55:17.878141500Z",
     "start_time": "2024-03-23T08:55:17.875603200Z"
    }
   },
   "id": "2bf4bae66369638b",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Experimental parameters\n",
    "label_data_rate = 0.1\n",
    "\n",
    "#Metrics\n",
    "metric1 = 'acc'\n",
    "metric2 = 'auc'\n",
    "\n",
    "#parameters to train the encoder\n",
    "p_m = 0.3\n",
    "alpha = 2.0\n",
    "K = 3\n",
    "beta = 1.0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T08:55:17.891833600Z",
     "start_time": "2024-03-23T08:55:17.879326900Z"
    }
   },
   "id": "27ba2315d4ab5346",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#load case dataset\n",
    "df = pd.read_excel(\"..\\Data\\wheelchair.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T08:55:22.784206700Z",
     "start_time": "2024-03-23T08:55:17.882833600Z"
    }
   },
   "id": "81b357376a247c3e",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X, y = one_hot_encode(df, 'class')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T08:55:22.795922500Z",
     "start_time": "2024-03-23T08:55:22.785358300Z"
    }
   },
   "id": "a4a575099b58a2f",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "x_unlab, x_train, x_test, y_train, y_test = split_data_with_preprocessing(X, y, label_data_rate, test_size=0.15)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T08:55:22.827512900Z",
     "start_time": "2024-03-23T08:55:22.796960300Z"
    }
   },
   "id": "80b358265d1c997a",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Alfiya\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Alfiya\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\Alfiya\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Alfiya\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "166/166 [==============================] - 1s 653us/step - loss: 1.0839 - mask_loss: 0.6408 - feature_loss: 0.2215\n",
      "Epoch 2/10\n",
      "166/166 [==============================] - 0s 612us/step - loss: 0.9154 - mask_loss: 0.6165 - feature_loss: 0.1495\n",
      "Epoch 3/10\n",
      "166/166 [==============================] - 0s 605us/step - loss: 0.8756 - mask_loss: 0.6157 - feature_loss: 0.1300\n",
      "Epoch 4/10\n",
      "166/166 [==============================] - 0s 625us/step - loss: 0.8594 - mask_loss: 0.6149 - feature_loss: 0.1223\n",
      "Epoch 5/10\n",
      "166/166 [==============================] - 0s 617us/step - loss: 0.8504 - mask_loss: 0.6143 - feature_loss: 0.1181\n",
      "Epoch 6/10\n",
      "166/166 [==============================] - 0s 609us/step - loss: 0.8441 - mask_loss: 0.6138 - feature_loss: 0.1151\n",
      "Epoch 7/10\n",
      "166/166 [==============================] - 0s 614us/step - loss: 0.8391 - mask_loss: 0.6134 - feature_loss: 0.1128\n",
      "Epoch 8/10\n",
      "166/166 [==============================] - 0s 614us/step - loss: 0.8352 - mask_loss: 0.6131 - feature_loss: 0.1110\n",
      "Epoch 9/10\n",
      "166/166 [==============================] - 0s 611us/step - loss: 0.8319 - mask_loss: 0.6129 - feature_loss: 0.1095\n",
      "Epoch 10/10\n",
      "166/166 [==============================] - 0s 615us/step - loss: 0.8292 - mask_loss: 0.6127 - feature_loss: 0.1083\n",
      "Proposed SLAB framework is trained.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Train SLAB-Self encoder\n",
    "slab_self_parameters = dict()\n",
    "slab_self_parameters['batch_size'] = 128\n",
    "slab_self_parameters['epochs'] = 10\n",
    "slab_valence_encoder = hexr_self(x_unlab, p_m, alpha, slab_self_parameters)\n",
    "    \n",
    "# Save encoder\n",
    "if not os.path.exists('Models'):\n",
    "  os.makedirs('Models')\n",
    "\n",
    "file_name = 'Models/wheel_80_5.h5'\n",
    "    \n",
    "slab_valence_encoder.save(file_name)  "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T08:55:24.653193600Z",
     "start_time": "2024-03-23T08:55:22.828513300Z"
    }
   },
   "id": "30728c5f68f65868",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "74/74 [==============================] - 0s 409us/step\n",
      "130/130 [==============================] - 0s 419us/step\n"
     ]
    }
   ],
   "source": [
    "#predict and evaluate slab self encoder\n",
    "slab_valence_encoder = load_model('Models/wheel_80_5.h5')\n",
    "\n",
    "# Test SLAB\n",
    "x_train_hat = slab_valence_encoder.predict(x_train)\n",
    "x_test_hat = slab_valence_encoder.predict(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T08:55:24.881167800Z",
     "start_time": "2024-03-23T08:55:24.654220700Z"
    }
   },
   "id": "7af198a460b27596",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from utils import convert_vector_to_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "\n",
    "def mlp(x_train, y_train, x_test, parameters):\n",
    "  \"\"\"Multi-layer perceptron (MLP).\n",
    "\n",
    "  Args:\n",
    "    - x_train, y_train: training dataset\n",
    "    - x_test: testing feature\n",
    "    - parameters: hidden_dim, epochs, activation, batch_size, num_layers\n",
    "\n",
    "  Returns:\n",
    "    - y_test_hat: predicted values for x_test\n",
    "  \"\"\"\n",
    "\n",
    "  # Convert labels into proper format\n",
    "  if len(y_train.shape) == 1:\n",
    "    y_train = convert_vector_to_matrix(y_train)\n",
    "\n",
    "  # Divide training and validation sets (9:1)\n",
    "  idx = np.random.permutation(len(x_train[:, 0]))\n",
    "  train_idx = idx[:int(len(idx)*0.9)]\n",
    "  valid_idx = idx[int(len(idx)*0.9):]\n",
    "\n",
    "  # Validation set\n",
    "  x_valid = x_train[valid_idx, :]\n",
    "  y_valid = y_train[valid_idx, :]\n",
    "\n",
    "  # Training set\n",
    "  x_train = x_train[train_idx, :]\n",
    "  y_train = y_train[train_idx, :]\n",
    "\n",
    "  # Reset the graph\n",
    "  K.clear_session()\n",
    "\n",
    "  # Define network parameters\n",
    "  hidden_dim = parameters['hidden_dim']\n",
    "  epochs_size = parameters['epochs']\n",
    "  act_fn = parameters['activation']\n",
    "  batch_size = parameters['batch_size']\n",
    "  #std_value = parameters['std_value']\n",
    "\n",
    "  # Define basic parameters\n",
    "  data_dim = len(x_train[0, :])\n",
    "  label_dim = len(y_train[0, :])\n",
    "\n",
    "  # Build model - with number of layers specified by parameters\n",
    "  model = Sequential()\n",
    "  initializer = HeNormal()\n",
    "  \n",
    "  model.add(Dense(hidden_dim, input_dim = data_dim, activation = act_fn, kernel_initializer=initializer))\n",
    "  for i in range(0, parameters['num_layers']):\n",
    "    model.add(Dense(hidden_dim, activation = act_fn, kernel_initializer=initializer))\n",
    "  model.add(Dense(label_dim, activation = 'softmax'))\n",
    "\n",
    "  model.compile(loss = 'categorical_crossentropy', optimizer='adam',\n",
    "                metrics = ['acc'])\n",
    "\n",
    "  es = EarlyStopping(monitor='val_loss', mode = 'min',\n",
    "                     verbose = 1, restore_best_weights=True, patience=50)\n",
    "\n",
    "  # Fit model on training dataset\n",
    "  model.fit(x_train, y_train, validation_data = (x_valid, y_valid),\n",
    "            epochs = epochs_size, batch_size = batch_size,\n",
    "            verbose = 0, callbacks=[es])\n",
    "\n",
    "  # Predict on x_test\n",
    "  y_test_hat = model.predict(x_test)\n",
    "  #print(\"Random Normal initialization\")\n",
    "  #print(\"SLAB MLP was trained on the test.\")\n",
    "  return y_test_hat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T08:55:24.887529700Z",
     "start_time": "2024-03-23T08:55:24.884168Z"
    }
   },
   "id": "a0bf19b23e1addd2",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#experiment to compute accuracy for different number of hidden layers\n",
    "wc_acc = []\n",
    "wc_auc = []\n",
    "f1_score_weg = []\n",
    "\n",
    "mlp_parameters = dict()\n",
    "mlp_parameters['hidden_dim'] = 100\n",
    "mlp_parameters['epochs'] = 100\n",
    "mlp_parameters['activation'] = 'relu'\n",
    "mlp_parameters['batch_size'] = 128\n",
    "mlp_parameters['num_layers'] = 5"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T08:55:24.890400300Z",
     "start_time": "2024-03-23T08:55:24.888892100Z"
    }
   },
   "id": "7afaf62c158472d3",
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for i in range (0, 5):\n",
    "        y_test_hat = mlp(x_train_hat, y_train, x_test_hat, mlp_parameters)\n",
    "        wc_acc.append(perf_metric(metric1, y_test, y_test_hat))\n",
    "        wc_auc.append(perf_metric(metric2, y_test, y_test_hat)) \n",
    "        f1_score_weg.append(f1_score(y_test.argmax(1),y_test_hat.argmax(1),average=\"weighted\"))"
   ],
   "id": "3d6a018c5b15b4c4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of valence accuracies for SLAB is  0.707\n",
      "Standard Deviation of Valence Accuracies using SLAB:  0.0059\n",
      "Mean of valence accuracies AUC Score for SLAB is  0.9475\n",
      "Mean of f1-score weighted for SLAB is  0.704\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean of valence accuracies for SLAB is \",round(np.mean(wc_acc),4))\n",
    "\n",
    "# Compute standard deviation\n",
    "std_dev_acc = np.std(wc_acc)\n",
    "print(\"Standard Deviation of Valence Accuracies using SLAB: \", round(std_dev_acc,4))\n",
    "    \n",
    "print(\"Mean of valence accuracies AUC Score for SLAB is \",round(np.mean(wc_auc),4))\n",
    "\n",
    "#print(\"Mean of f1-score mic for SLAB is \",round(np.mean(top_score_mic),4))\n",
    "#print(\"Mean of f1-score mac for SLAB is \",round(np.mean(top_score_mac),4))\n",
    "print(\"Mean of f1-score weighted for SLAB is \",round(np.mean(f1_score_weg),4))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T09:15:19.463518900Z",
     "start_time": "2024-03-23T09:15:19.458320Z"
    }
   },
   "id": "af4f12e6f9dd6b02",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T09:05:01.678495100Z",
     "start_time": "2024-03-23T09:05:01.676504900Z"
    }
   },
   "id": "8d8983729e0fa187",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
